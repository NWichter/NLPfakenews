{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Data and create training- and testdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "\n",
    "    if os.path.exists(file_name): #Kontrolle, ob die Daten vorhanden sind\n",
    "            with open(file_name, 'r', encoding=\"utf8\") as csvfile: #Öffnen und auslesen der Datei \n",
    "                csv_reader_object = csv.reader(csvfile, delimiter=',')\n",
    "                counter = 0\n",
    "                csv_list = []\n",
    "                for row in csv_reader_object: #Jede Zeile in den Daten wird ausgelesen und in einer Liste gespeichert\n",
    "                    #print(row)\n",
    "                    if counter == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        csv_list.append(row+[file_name[7:11]])\n",
    "                    counter += 1\n",
    "            \n",
    "            #print(counter-1,\"Einträge aus\", file_name[7:], \"geladen\")\n",
    "            return csv_list        \n",
    "    else:\n",
    "        print(\"Datei\", file_name ,\"nicht gefunden\") \n",
    "        \n",
    "def get_data():\n",
    "    file_folder = \"./data/\"\n",
    "    files = [\"Fake.csv\",\"True.csv\"]\n",
    "\n",
    "    main_data = []\n",
    "\n",
    "    for element in files: #Jede Datei aus der Liste wird ausgelesen\n",
    "        file_name = file_folder+element\n",
    "        main_data += (load_data(file_name))\n",
    "\n",
    "    #print(\"Es gibt insgesamt\", len(main_data), \"Einträge\")\n",
    "\n",
    "    random.shuffle(main_data) #Randomizieren aller Daten\n",
    "    main_data,unused_data = train_test_split(main_data,test_size=0.90) #Reduzierung des Datensatz auf 30%\n",
    "    train_data,test_data = train_test_split(main_data,test_size=0.2) #Unterteilung in Training- und Testdaten\n",
    "\n",
    "    #print(\"Länge train_data:\", len(train_data),\" und Länge test_data:\", len(test_data))\n",
    "    #print(train_data[0][3])\n",
    "\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data(data):\n",
    "\n",
    "    qwe_list = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n1234567890“”,'‘’…¿0\\\\\"\n",
    "    forbidden_words = [\"pictwitter\",\"http\",\"reuters\",\"\\xa0\"]\n",
    "    replace_words = []\n",
    "    data_clear = []\n",
    "    \n",
    "    for news in data: #Für jeden Datensatz werden Zahlen und Sonderzeichen herausgefilter und in einzelne Wörter unterteilt\n",
    "        record = news[0]+news[1]\n",
    "        for element in qwe_list:\n",
    "            record = record.replace(element,\" \")\n",
    "        record = record.split(\" \")\n",
    "\n",
    "        tmp_list = []\n",
    "        for element in record: #Für jedes Wort im Datensatz wird kontrolliert, ob es erlaubt ist und in Kleinbuchstaben gesetzt\n",
    "            element = element.replace(\" \",\"\").lower()\n",
    "            skip = 0\n",
    "            for fbw in forbidden_words:\n",
    "                if fbw in element:\n",
    "                    skip = 1\n",
    "                    break                    \n",
    "            \n",
    "            if len(element)>1 and skip == 0 : #jedes Wort muss eine minimal Länge von 2 Zeichen besitzen\n",
    "                tmp_list.append(element)\n",
    "            \n",
    "        news = [tmp_list,news[4]]\n",
    "        data_clear.append(news)\n",
    "    return data_clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_train(train_data): #Umwandlung der Wörter im Trainingdatensatz in Wahrscheinlichkeiten\n",
    "    all_word_count = {}\n",
    "    news_count = len(train_data) \n",
    "    news_tf = {}\n",
    "    unique_words = []\n",
    "    index = 0\n",
    "    \n",
    "    for news in train_data: \n",
    "        #Für jeden Datensatz der Trainingsdaten wird ein temporäres Dict erstellt \n",
    "        #in welchem die Anzahl der vorkommenden Wörter gespeichert wird.\n",
    "        len_news = len(news[0])\n",
    "        tmp_word_count = {}\n",
    "        news_tf.update({index:{}})\n",
    "                      \n",
    "        for word in news[0]:\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "            \n",
    "            if word in tmp_word_count.keys():\n",
    "                tmp_word_count[word] += 1\n",
    "            else:\n",
    "                tmp_word_count.update({word:1})\n",
    "                \n",
    "        for word in tmp_word_count.keys():\n",
    "            \n",
    "            news_tf[index].update({word:tmp_word_count[word]/len_news}) #{word:tf}\n",
    "            \n",
    "            if word in all_word_count.keys():\n",
    "                all_word_count[word] += 1\n",
    "            else:\n",
    "                all_word_count.update({word:1})\n",
    "                \n",
    "        index += 1\n",
    "\n",
    "    idf_dict = {} #{word:idf}\n",
    "    for word in all_word_count.keys():\n",
    "        df = all_word_count[word]/news_count #df\n",
    "        idf = math.log(news_count/(df+1),10)\n",
    "        idf_dict.update({word:idf})\n",
    "        #tf_idf = tf * idf\n",
    "\n",
    "    unique_words.sort() #Alphanumerisches Sortieren des Datensatz\n",
    "    unique_word_count = len(unique_words) #Länge der Liste mit einmaligen Wörtern\n",
    "    \n",
    "    #print(unique_words) #Alle Wörter, welche in allen Texten vorkommen\n",
    "    #print(news_count) #Anzahl Nachrichtentexte\n",
    "    #print(news_tf.keys()) #Dict mit Wort und tf-Wert\n",
    "    \n",
    "    #Shape der Matrix = news_count*unique_word_count+1 | das +1 ist für den class_value(True/Fake)(1/0)\n",
    "    tf_idf_matrix = np.zeros((news_count, unique_word_count+1), dtype=float) \n",
    "    # Erstellen einer Nullmatrix mit Anzahl Texte*Anzahl Wörter\n",
    "    \n",
    "    print(len(all_word_count.keys()))\n",
    "    \n",
    "    for news_index in range(news_count):\n",
    "        if train_data[news_index][-1] == \"True\":\n",
    "            tf_idf_matrix[news_index,unique_word_count] = 1\n",
    "            \n",
    "        for word_index in range(unique_word_count):\n",
    "            if unique_words[word_index] in news_tf[news_index].keys():\n",
    "                tf_idf_matrix[news_index,word_index] = news_tf[news_index][unique_words[word_index]] * idf_dict[unique_words[word_index]]\n",
    "\n",
    "    return tf_idf_matrix,news_count,all_word_count\n",
    "\n",
    "#idf   \n",
    "#tf(t,d) = count of t in d / number of unique_words in d\n",
    "#df(t) = occurrence of t in documents\n",
    "#idf(t) = N/df\n",
    "#idf(t) = log(N/(df + 1))\n",
    "#tf-idf(t, d) = tf(t, d) * log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = get_data()\n",
    "train_data_clear = clear_data(train_data)\n",
    "test_data_clear = clear_data(test_data)\n",
    "\n",
    "training_data_tfidf, total_rows,all_word_count = tf_idf_train(train_data_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41707\n",
      "898 41708\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e9e732d699b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mtf_idf_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnews_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews_tf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnews_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0midf_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf_idf_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mtf_idf_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_word_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-e9e732d699b9>\u001b[0m in \u001b[0;36mtf_idf_test\u001b[1;34m(test_data, all_word_count)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_word_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnews_tf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnews_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0munique_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnews_tf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnews_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mtf_idf_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnews_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews_tf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnews_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0midf_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "def tf_idf_test(test_data,all_word_count):#Umwandlung der Wörter im Testdatensatz in Wahrscheinlichkeiten\n",
    "    news_count = len(test_data) \n",
    "    news_tf = {}\n",
    "    index = 0\n",
    "    unique_words = all_word_count.keys()\n",
    "    unique_word_count = len(unique_words)\n",
    "    \n",
    "    for news in train_data: \n",
    "        #Für jeden Datensatz der Testdaten wird ein temporäres Dict erstellt \n",
    "        #in welchem die Anzahl der vorkommenden Wörter gespeichert wird.\n",
    "        len_news = len(news[0])\n",
    "        tmp_word_count = {}\n",
    "        news_tf.update({index:{}}) \n",
    "        \n",
    "        for word in news[0]: \n",
    "            if word not in unique_words: #Wörter, welche nicht im Trainingsdatensatz vorkommen werden aussortiert\n",
    "                break\n",
    "\n",
    "            if word in tmp_word_count.keys():\n",
    "                tmp_word_count[word] += 1\n",
    "            else:\n",
    "                tmp_word_count.update({word:1})\n",
    "\n",
    "            for word in tmp_word_count.keys():\n",
    "\n",
    "                news_tf[index].update({word:tmp_word_count[word]/len_news}) #{word:tf}\n",
    "                all_word_count[word] += 1\n",
    "                #if word in all_word_count.keys():       \n",
    "            index += 1\n",
    "\n",
    "    idf_dict = {} #{word:idf}\n",
    "    for word in all_word_count.keys():\n",
    "        df = all_word_count[word]/news_count #df\n",
    "        idf = math.log(news_count/(df+1),10)\n",
    "        idf_dict.update({word:idf})\n",
    "        \n",
    "    print(len(all_word_count.keys()))    \n",
    "        \n",
    "    print(news_count, unique_word_count+1)        \n",
    "    \n",
    "    tf_idf_matrix = np.zeros((news_count, unique_word_count+1), dtype=float)\n",
    "    for news_index in range(news_count):\n",
    "            \n",
    "        for word_index in range(unique_word_count):\n",
    "            print(unique_words[word_index],news_tf[news_index].keys())\n",
    "            if unique_words[word_index] in news_tf[news_index].keys():\n",
    "                tf_idf_matrix[news_index,word_index] = news_tf[news_index][unique_words[word_index]] * idf_dict[unique_words[word_index]]\n",
    "    return tf_idf_matrix\n",
    "tf_idf_test(test_data,all_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(dataset): #Berechnung des Durschnittswert, Standardbweichung und Anzahl im Datensatz\n",
    "    stats = [(np.mean(row),np.std(row),len(row)) for row in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "def naives_bayes(tf_idf_matrix): #Anwendung des Naives Bayes Algorithmus \n",
    "    \n",
    "    separated = {0:[],1:[]}\n",
    "    summaries = {}\n",
    "    \n",
    "    for element in range(len(tf_idf_matrix)): #Aufteilen des Datensatz in True und Fake\n",
    "        vector = tf_idf_matrix[element]\n",
    "        class_value = vector[-1]\n",
    "        separated[class_value].append(vector[:-1:])\n",
    "    \n",
    "    for class_value, rows in separated.items(): #Speichern \n",
    "        summaries[class_value] = summarize_dataset(rows)\n",
    "\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(x, mean, std): #Berechnung der (Gaussian) probability distribution function\n",
    "    exponent = math.exp(-((x-mean)**2 / (2 * std**2 )))\n",
    "    return (1 / (math.sqrt(2 * math.pi) * std)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(total_rows,summaries,new_news):\n",
    "    probabilities = {}\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "        for element in range(len(class_summaries)):\n",
    "            mean, std, count = class_summaries[element]\n",
    "            probabilities[class_value] *= pdf(new_news[element], mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage der class_value\n",
    "def predict(summaries, row):\n",
    "    probabilities = calculate_class_probabilities(summaries, row)\n",
    "    best_label, best_prob = None, -1\n",
    "    \n",
    "    for class_value, probability in probabilities.items():\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(train_data, test_data): #Lernen des Algorithmus und Testen des Algorithmus mit Testdaten\n",
    "    \n",
    "    summarize = summarize_by_class(train_data) #Trainieren des Algorithmus mit Testdaten\n",
    "    predictions = []\n",
    "    for row in test_data: #Testen des Algortihmus mit Testdaten\n",
    "        predictions.append(predict(summarize, row))\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_values(data): #Herausfiltern der class_values aus dem Testdatensatz\n",
    "    test_class_values = []\n",
    "    for element in data:\n",
    "        test_class_values.append(element[1])\n",
    "    return test_class_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_class_values, predictions): # Ermitteln der Genauigkeit des Algorithmus\n",
    "    fp,tp,fn,tn = 0,0,0,0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ausführen der Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = get_data()\n",
    "train_data_clear = clear_data(train_data)\n",
    "test_data_clear = clear_data(test_data)\n",
    "test_class_values = get_class_values(test_data_clear)\n",
    "\n",
    "training_data_tfidf, total_rows,all_word_count = tf_idf_train(train_data_clear)\n",
    "test_data_tfidf= tf_idf_test(test_data,all_word_count)\n",
    "\n",
    "predictions = test(tf_idf_matrix,test_data_tfidf)\n",
    "\n",
    "accuracy(test_class_values, predictions)\n",
    "#summaries = naives_bayes(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
