{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Data and create training- and testdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "\n",
    "    if os.path.exists(file_name): #Kontrolle, ob die Daten vorhanden sind\n",
    "            with open(file_name, 'r', encoding=\"utf8\") as csvfile: #Öffnen und auslesen der Datei \n",
    "                csv_reader_object = csv.reader(csvfile, delimiter=',')\n",
    "                counter = 0\n",
    "                csv_list = []\n",
    "                for row in csv_reader_object: #Jede Zeile in den Daten wird ausgelesen und in einer Liste gespeichert\n",
    "                    #print(row)\n",
    "                    if counter == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        csv_list.append(row+[file_name[7:11]])\n",
    "                    counter += 1\n",
    "            \n",
    "            #print(counter-1,\"Einträge aus\", file_name[7:], \"geladen\")\n",
    "            return csv_list        \n",
    "    else:\n",
    "        print(\"Datei\", file_name ,\"nicht gefunden\") \n",
    "        \n",
    "def get_data():\n",
    "    file_folder = \"./data/\"\n",
    "    files = [\"Fake.csv\",\"True.csv\"]\n",
    "\n",
    "    main_data = []\n",
    "\n",
    "    for element in files: #Jede Datei aus der Liste wird ausgelesen\n",
    "        file_name = file_folder+element\n",
    "        main_data += (load_data(file_name))\n",
    "\n",
    "    #print(\"Es gibt insgesamt\", len(main_data), \"Einträge\")\n",
    "\n",
    "    #random.shuffle(main_data) #Randomizieren aller Daten\n",
    "    main_data,unused_data = train_test_split(main_data,test_size=0.9) #Reduzierung des Datensatz auf 10%\n",
    "    train_data,test_data = train_test_split(main_data,test_size=0.2) #Unterteilung in Training- und Testdaten\n",
    "\n",
    "    print(\"Länge train_data:\", len(train_data),\" und Länge test_data:\", len(test_data))\n",
    "    #print(train_data[0][3])\n",
    "\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_data(data):\n",
    "\n",
    "    qwe_list = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n1234567890“”,'‘’…¿0\\\\\"\n",
    "    forbidden_words = [\"pictwitter\",\"http\",\"reuters\",\"\\\\xa0\",\"\\\\u200\"]\n",
    "    data_clear = []\n",
    "    \n",
    "    for news in data: #Für jeden Datensatz werden Zahlen und Sonderzeichen herausgefilter und in einzelne Wörter unterteilt\n",
    "        try:        \n",
    "            record = news[0]+news[1]\n",
    "        except:\n",
    "            record = news[0]\n",
    "        for element in qwe_list:\n",
    "            record = record.replace(element,\" \")\n",
    "        record = record.split(\" \")\n",
    "\n",
    "        tmp_list = []\n",
    "        for element in record: #Für jedes Wort im Datensatz wird kontrolliert, ob es erlaubt ist und in Kleinbuchstaben gesetzt\n",
    "            element = element.replace(\" \",\"\").lower()\n",
    "            skip = 0\n",
    "            for fbw in forbidden_words:\n",
    "                if fbw in element:\n",
    "                    skip = 1\n",
    "                    break                    \n",
    "            \n",
    "            if len(element)>1 and skip == 0 : #jedes Wort muss eine minimal Länge von 2 Zeichen besitzen\n",
    "                tmp_list.append(element)\n",
    "        try:    \n",
    "            news = [tmp_list,news[4]]\n",
    "        except:\n",
    "            news = [tmp_list]\n",
    "        data_clear.append(news)\n",
    "    return data_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge train_data: 3591  und Länge test_data: 898\n"
     ]
    }
   ],
   "source": [
    "#Laden und Aufbreiten der Training- und Testdaten\n",
    "train_data,test_data = get_data()\n",
    "train_data_clear = clear_data(train_data)\n",
    "test_data_clear = clear_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancetest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance (True/False): 90% - 110% is good\t >100% => more True\n",
      "Training: 86.55%\n",
      "Test: 88.66%\n"
     ]
    }
   ],
   "source": [
    "train_true = 0\n",
    "train_false = 0\n",
    "test_true = 0\n",
    "test_false = 0\n",
    "\n",
    "for x in range(len(train_data_clear)):\n",
    "    if train_data_clear[x][1] == \"Fake\":\n",
    "        train_false+= 1\n",
    "    if train_data_clear[x][1] == \"True\":\n",
    "        train_true+= 1\n",
    "        \n",
    "for x in range(len(test_data_clear)):\n",
    "    if test_data_clear[x][1] == \"Fake\":\n",
    "        test_false+= 1\n",
    "    if test_data_clear[x][1] == \"True\":\n",
    "        test_true+= 1\n",
    "    \n",
    "print(f\"Balance (True/False): 90% - 110% is good\\t >100% => more True\\nTraining: {round(train_true/train_false*100,2)}%\\nTest: {round(test_true/test_false*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_train(train_data): #Umwandlung der Wörter im Trainingdatensatz in Wahrscheinlichkeiten\n",
    "    all_word_count = {} #Dict für alle Wörter in allen Texten\n",
    "    news_count = len(train_data) #Anzahl der Nachrichtentexte\n",
    "    news_tf = {} #Dict für tf-Werte pro Wort je Nachrichtentext {0:{word:tf-Wert,...}...}\n",
    "    unique_words = [] #Liste aller vorkommenden Wörter\n",
    "    index = 0 #Index für den news_tf\n",
    "    \n",
    "    for news in train_data: \n",
    "        len_news = len(news[0]) #Länge des aktuellen Nachrichtentext\n",
    "        tmp_word_count = {} #Dict für alle in Wörter in einem Nachrichtentext\n",
    "        news_tf.update({index:{}}) #Hinzufügen eines neues Dict für einen Nachrichtentext\n",
    "                      \n",
    "        for word in news[0]: #Für jedes Wort im Nachrichtentext\n",
    "            if word not in unique_words: #Überprüfen, ob das Wort noch nicht in unique_words vorkommt, falls ja hinzufügen dieses\n",
    "                unique_words.append(word)\n",
    "            \n",
    "            if word in tmp_word_count.keys():\n",
    "                tmp_word_count[word] += 1\n",
    "            else:\n",
    "                tmp_word_count.update({word:1})\n",
    "                \n",
    "        for word in tmp_word_count.keys(): #Temporäres Dict für jeden Nachrichtentext mit Anzahl der vorkommenden Wörter\n",
    "            \n",
    "            news_tf[index].update({word:tmp_word_count[word]/len_news}) #{word:tf}\n",
    "            \n",
    "            if word in all_word_count.keys():\n",
    "                all_word_count[word] += 1\n",
    "            else:\n",
    "                all_word_count.update({word:1})\n",
    "                \n",
    "        index += 1\n",
    "\n",
    "    idf_dict = {} #{word:idf}\n",
    "    for word in all_word_count.keys(): \n",
    "        df = all_word_count[word]/news_count #df\n",
    "        idf = math.log(news_count/(df+1),10)\n",
    "        idf_dict.update({word:idf})\n",
    "        #tf_idf = tf * idf\n",
    "\n",
    "    unique_words.sort() #Alphanumerisches Sortieren des Datensatz\n",
    "    unique_word_count = len(unique_words) #Länge der Liste mit einmaligen Wörtern\n",
    "    \n",
    "    #Shape der Matrix = news_count*unique_word_count+1 | das +1 ist für den class_value(True/Fake)(1/0)\n",
    "    tf_idf_matrix = np.zeros((news_count, unique_word_count+1), dtype=float) \n",
    "    # Erstellen einer Nullmatrix mit Anzahl Texte*Anzahl Wörter\n",
    "    \n",
    "    for news_index in range(news_count):\n",
    "        if train_data[news_index][-1] == \"True\":\n",
    "            tf_idf_matrix[news_index,unique_word_count] = 1            \n",
    "        for word_index in range(unique_word_count):\n",
    "            #print([news_index,word_index])\n",
    "            if unique_words[word_index] in news_tf[news_index].keys():\n",
    "                tf_idf_matrix[news_index,word_index] = news_tf[news_index][unique_words[word_index]] * idf_dict[unique_words[word_index]]\n",
    "\n",
    "    return tf_idf_matrix,all_word_count,news_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_test(test_data,all_word_count,total_rows):#Umwandlung der Wörter im Testdatensatz in Wahrscheinlichkeiten\n",
    "    news_count = len(test_data) \n",
    "    news_tf = {}\n",
    "    index = 0\n",
    "    unique_words = list(all_word_count.keys())\n",
    "    unique_word_count = len(unique_words)\n",
    "    \n",
    "    for news in test_data: \n",
    "        #Für jeden Datensatz der Testdaten wird ein temporäres Dict erstellt \n",
    "        #in welchem die Anzahl der vorkommenden Wörter gespeichert wird.\n",
    "        len_news = len(news[0])\n",
    "        tmp_word_count = {}\n",
    "        news_tf.update({index:{}}) \n",
    "        \n",
    "        for word in news[0]:\n",
    "            \n",
    "            if word not in unique_words: #Wörter, welche nicht im Trainingsdatensatz vorkommen werden aussortiert\n",
    "                continue\n",
    "\n",
    "            if word in tmp_word_count.keys():\n",
    "                tmp_word_count[word] += 1\n",
    "            else:\n",
    "                tmp_word_count.update({word:1})\n",
    "\n",
    "        for word in tmp_word_count.keys(): #Temporäres Dict für jeden Nachrichtentext mit Anzahl der vorkommenden Wörter\n",
    "            \n",
    "            news_tf[index].update({word:tmp_word_count[word]/len_news}) #{word:tf}\n",
    "            \n",
    "            if word in all_word_count.keys():\n",
    "                all_word_count[word] += 1\n",
    "\n",
    "        index += 1\n",
    "    \n",
    "    idf_dict = {} #{word:idf}\n",
    "    total_rows += news_count\n",
    "    \n",
    "    for word in all_word_count.keys():\n",
    "        df = all_word_count[word]/(total_rows) #df\n",
    "        idf = math.log((total_rows)/(df+1),10)\n",
    "        idf_dict.update({word:idf})   \n",
    "        \n",
    "    tf_idf_matrix = np.zeros((news_count, unique_word_count), dtype=float)\n",
    "    for news_index in range(news_count):\n",
    "        for word_index in range(unique_word_count):\n",
    "            if unique_words[word_index] in news_tf[news_index].keys():\n",
    "                tf_idf_matrix[news_index,word_index] = news_tf[news_index][unique_words[word_index]] * idf_dict[unique_words[word_index]]\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umwandeln des Training- und Testdatensatz ins TF-IDF-Format\n",
    "tf_idf_train, all_word_count,total_rows = get_tf_idf_train(train_data_clear)\n",
    "tf_idf_test = get_tf_idf_test(test_data_clear,all_word_count,total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "counter_words = 0\n",
    "\n",
    "for element in tf_idf_test[0]:\n",
    "    if element>0:\n",
    "        #print(f\"{counter}: {element}\")\n",
    "        counter_words += 1\n",
    "\n",
    "    counter += 1\n",
    "#print(f\"Gesamtwörter im Text: {counter_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern des TF-IDF (verworfen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(filename, \"w\")\n",
    "# json.dump(dic, f)\n",
    "# f = open(filename, \"r\")\n",
    "#     dic = json.loads(f.read())\n",
    "    \n",
    "# np.save('word_vec_dict.npy', word_vec_dict)\n",
    "# np.load('word_vec_dict.npy', allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naives Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(dataset): #Berechnung des Durschnittswert, Standardbweichung und Anzahl im Datensatz\n",
    "    #print(dataset)\n",
    "    stats = []\n",
    "    for row in zip(*dataset):\n",
    "        \n",
    "        mean = np.mean(row)\n",
    "        std = np.std(row)\n",
    "        \n",
    "        if mean<=0:\n",
    "            mean = 4.8e-6\n",
    "            \n",
    "        if std <=0:\n",
    "            std = 4.8e-8\n",
    "\n",
    "        stats.append([mean,std,len(row)])\n",
    "    \n",
    "    #print(\"stats: \",stats)\n",
    "    #print(stats)\n",
    "    return stats\n",
    "\n",
    "def naives_bayes(tf_idf_matrix): #Anwendung des Naives Bayes Algorithmus \n",
    "    \n",
    "    separated = {0:[],1:[]}\n",
    "    summaries = {}\n",
    "    \n",
    "    for element in range(len(tf_idf_matrix)): #Aufteilen des Datensatz in True und Fake\n",
    "        vector = tf_idf_matrix[element]\n",
    "        #print(\"vector: \", vector)\n",
    "        class_value = vector[-1]\n",
    "        separated[class_value].append(vector[:-1:])\n",
    "    \n",
    "    for class_value, rows in separated.items(): #Speichern der Datensätze in einem Dict\n",
    "        summaries[class_value] = summarize_dataset(rows)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(x, mean, std): #Berechnung der (Gaussian) probability distribution function\n",
    "    stdx = 2*std**2\n",
    "    #print(\"std: \",std)\n",
    "    #print(\"stdx: \",stdx)\n",
    "    exponent = math.exp(-((x-mean)**2 / stdx))\n",
    "    if exponent == 0:\n",
    "        return 1\n",
    "    #print(\"exponent: \",exponent)\n",
    "    pdf = (1 / (math.sqrt(2 * math.pi) * std)) * exponent\n",
    "    #print(\"pdf: \",pdf) \n",
    "    return pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(summaries,new_news):\n",
    "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\n",
    "    probabilities = {}\n",
    "    for class_value, class_summaries in summaries.items():\n",
    "        probabilities[class_value] = summaries[class_value][0][2]/total_rows\n",
    "        #print(class_value,probabilities[class_value],summaries[class_value])\n",
    "        for element in range(len(class_summaries)):\n",
    "            mean, std, count = class_summaries[element]\n",
    "            #print(new_news[element])\n",
    "            probabilities[class_value] *= pdf(new_news[element], mean, std)\n",
    "    #print(probabilities)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage der class_value des Testdatensatz\n",
    "def predict(summaries, new_news):\n",
    "    \n",
    "    probabilities = probs(summaries, new_news)\n",
    "    best_label, best_prob = None, -1\n",
    "    \n",
    "    for class_value, probability in probabilities.items():\n",
    "        #print(probability,\" : \",class_value)\n",
    "        if best_label is None or probability > best_prob:\n",
    "            best_prob = probability\n",
    "            best_label = class_value\n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_data):\n",
    "    model = naives_bayes(train_data) #Trainieren des Algorithmus mit Testdaten\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(train_data, test_data): #Lernen des Algorithmus und Testen des Algorithmus mit Testdaten\n",
    "    \n",
    "    model = create_model(train_data)\n",
    "    \n",
    "    predictions = []\n",
    "    for new_news in test_data: #Testen des Algortihmus mit Testdaten\n",
    "        predictions.append(predict(model,new_news))\n",
    "    return predictions,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_values(data): #Herausfiltern der class_values aus dem Testdatensatz\n",
    "    test_class_values = []\n",
    "    for element in data:\n",
    "        test_class_values.append(element[1])\n",
    "    return test_class_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-db87105714db>:11: RuntimeWarning: overflow encountered in double_scalars\n",
      "  probabilities[class_value] *= pdf(new_news[element], mean, std)\n"
     ]
    }
   ],
   "source": [
    "predictions,model = test(tf_idf_train,tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_values = get_class_values(test_data_clear)\n",
    "test_classes = []\n",
    "for element in test_class_values:\n",
    "    if element == \"Fake\":\n",
    "        test_classes.append(0)\n",
    "    if element == \"True\":\n",
    "        test_classes.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_classes, predictions): # Evaluieren des Algorithmus\n",
    "    fp,tp,fn,tn = 0,0,0,0\n",
    "    #print(predictions)\n",
    "    #print(test_classes[:10],predictions[:10])\n",
    "    for i in range(len(test_classes)): # Zuweisung zu Wahrheitswerten\n",
    "        class_value = test_classes[i]\n",
    "        pred = predictions[i]\n",
    "        if class_value == 1 and pred == 1:\n",
    "            tp+= 1\n",
    "        elif class_value == 1 and pred == 0:\n",
    "            fn+= 1\n",
    "\n",
    "        elif class_value == 0 and pred == 0:\n",
    "            tn+= 1\n",
    "\n",
    "        elif class_value == 0 and pred == 1:\n",
    "            fp+= 1\n",
    "\n",
    "    # Berechnung der Evaluationwerte\n",
    "    print(\"Gesamt Nachrichtentexte:\", fp+tp+fn+tn)\n",
    "    print(f\"FP: {fp}: TP : {tp} FN: {fn} TN: {tn}\")\n",
    "    \n",
    "    if fp+tp+fn+tn > 0:\n",
    "        accuracy = round((tp+tn)/(fp+tp+fn+tn),3) \n",
    "    else:\n",
    "        accuracy = 0\n",
    "        \n",
    "    if tp+fp > 0:\n",
    "        precision = (tp)/(tp+fp)\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    if tp+fn > 0:\n",
    "        recall = (tp)/(tp+fn)\n",
    "    else:\n",
    "        recall = 0\n",
    "        \n",
    "    if precision+recall > 0:\n",
    "        f1score = 2*(precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        f1score = 0\n",
    "        \n",
    "    print(f\"Genauigkeit: {round(accuracy*100,3)}%\\nPräzision: {round(precision,3)}\\nRecall: {round(recall,3)}\\nF1-Score: {round(f1score,3)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamt Nachrichtentexte: 898\n",
      "FP: 52: TP : 53 FN: 369 TN: 424\n",
      "Genauigkeit: 53.1%\n",
      "Präzision: 0.505\n",
      "Recall: 0.126\n",
      "F1-Score: 0.201\n"
     ]
    }
   ],
   "source": [
    "evaluation(test_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosinus-Ähnlichkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "for i in range(len(tf_idf_test)): #Für jeden Testvektor werden die ähnlichten Trainingsvektoren errechnet\n",
    "    line_s= list(tf_idf_test[i])\n",
    "    similarity_dict = {}\n",
    "    \n",
    "\n",
    "    for j in range(len(tf_idf_train)): #Iteriere über alle Trainingsvektoren\n",
    "\n",
    "        query_s = list(tf_idf_train[j])[:-1]\n",
    "        \n",
    "        sim = 1 - cosine(line_s, query_s)\n",
    "\n",
    "        similarity_dict[j] = sim\n",
    "\n",
    "    similarity_dict = {k: v for k, v in sorted(similarity_dict.items(), key=lambda item: item[1], reverse = True)}\n",
    "    keys = list(similarity_dict.keys())[:21]\n",
    "    fake = 0\n",
    "    real = 0\n",
    "    for k in keys:\n",
    "        \n",
    "        tmp_class_label = int(list(tf_idf_train[k])[-1:][0])\n",
    "        #print(tmp_class_label)\n",
    "        if tmp_class_label == 0:\n",
    "            fake += 1\n",
    "        elif tmp_class_label == 1:\n",
    "            real += 1\n",
    "    #print(\"F\",fake,\" R\", real)\n",
    "    if fake > real:\n",
    "        class_list.append(0)\n",
    "    else:\n",
    "        class_list.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Cosinus-Ähnlichkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_classes, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_tfidf(news,all_word_count,total_rows):\n",
    "    unique_words = list(all_word_count.keys())\n",
    "    unique_word_count = len(unique_words)\n",
    "    len_news = len(news)\n",
    "    tmp_word_count = {}\n",
    "    news_tf = ({0:{}}) \n",
    "        \n",
    "    for word in news[0]:\n",
    "\n",
    "        if word not in unique_words: #Wörter, welche nicht im Trainingsdatensatz vorkommen werden aussortiert\n",
    "            continue\n",
    "\n",
    "        if word in tmp_word_count.keys():\n",
    "            tmp_word_count[word] += 1\n",
    "        else:\n",
    "            tmp_word_count.update({word:1})\n",
    "\n",
    "    for word in tmp_word_count.keys(): #Temporäres Dict für jeden Nachrichtentext mit Anzahl der vorkommenden Wörter\n",
    "\n",
    "        news_tf[index].update({word:tmp_word_count[word]/len_news}) #{word:tf}\n",
    "\n",
    "        if word in all_word_count.keys():\n",
    "            all_word_count[word] += 1\n",
    "\n",
    "\n",
    "    idf_dict = {} #{word:idf}\n",
    "    total_rows += 1\n",
    "\n",
    "    for word in all_word_count.keys():\n",
    "        df = all_word_count[word]/(total_rows) #df\n",
    "        idf = math.log((total_rows)/(df+1),10)\n",
    "        idf_dict.update({word:idf})   \n",
    "\n",
    "    tf_idf_matrix = np.zeros((1, unique_word_count), dtype=float)\n",
    "    \n",
    "    for word_index in range(unique_word_count):\n",
    "        if unique_words[word_index] in news_tf[0].keys():\n",
    "            tf_idf_matrix[0,word_index] = news_tf[0][unique_words[word_index]] * idf_dict[unique_words[word_index]]\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendungsfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = ['RAND PAUL Picks Fight With Wrong Senator: Says Cruz Is “Pretty Much Done” In Senate Because He Won’t Get Along [VIDEO] Here s a newsflash Rand We re not looking for a guy who can win a popularity contest in Washington to be our next President. We re actually looking for a candidate who is willing to stand up to politicians who ve forgotten who they came to Washington DC to represent We really do love much of what Rand stands for, but his comments about Senator Cruz couldn t have been more off base.Sen. Rand Paul on Tuesday said fellow Republican presidential candidate Ted. Cruz is  done for  in the Senate. Ted has chosen to make this really personal and chosen to call people dishonest in leadership and call them names, which really goes against the decorum and also against the rules of the Senate, and as a consequence, he can t get anything done legislatively,  Paul told Fox News Radio.  He is pretty much done for and stifled and it s really because of personal relationships, or lack of personal relationships, and it is a problem. Don t get us wrong We happen to agree with Senator Rand Paul on a lot of things. Picking on one of the most courageous men in Washington DC however, was not a very good idea especially when there is video footage like this that can be easily accessed:Paul, a Kentucky Republican who has had the backing of his home-state senior senator, Majority Leader Mitch McConnell, despite some tough policy differences, had been asked about Cruz s inability to even muster the support of 11 senators to secure a roll call vote on a procedural motion designed to amend the continuing resolution to keep the government running.The Senate s set to vote on passage of that measure at 10 a.m. on Wednesday, the last day of the federal government s 2015 fiscal year. I approach things a little different, I am still just as hardcore in saying what we are doing, I just chose not to call people liars on the Senate floor and it s just a matter of different perspectives on how best to get to the end result,  Paul said in the interview.Paul backed McConnell s 2014 re-election bid. Cruz had accused McConnell of lying to him about the way forward for the revival of the Export-Import Bank in the Senate.Like Cruz, Paul opposes the CR advanced by McConnell to avoid a shutdown later in the week. But Paul has focused his criticism on the use of the stopgap spending vehicles. Paul would have rather seen more pressure put on Democrats to advance the dozen individual appropriation bills. I would defund not only Planned Parenthood but hundreds and hundreds of regulations, hundreds and hundreds of wasteful programs. I would take them all out, put them on the table and say  You know what Democrats, it doesn t take 60 votes to defund something, it s actually going to take 60 votes to fund any of these programs,  vote on them one at a time and we will see how many of these crazy programs get 60 votes. My guess would be very few, but that would take the courage to let the spending expire and start anew and let new programs all require 60 votes to pass,  Paul said.McConnell signaled Tuesday he intends to call additional votes on the regular fiscal 2016 spending bills, though there was no evidence Democrats accede to that proposal without a big budget agreement between the two parties. Via: Roll CallIf you want to see why Ted Cruz isn t the most popular guy in Washington DC, watch this video that was taken only two days ago on the Senate floor. Watch this, and you ll know why conservatives who are sick of politicians who leave their spines in their home states with their campaign promises love this guy:']\n",
    "#Fake\n",
    "\n",
    "example_data_clear = clear_data(example_data)\n",
    "\n",
    "single_tfidf_value = single_tfidf(example_data_clear,all_word_count,total_rows)[0]\n",
    "\n",
    "#print(single_tfidf_value)\n",
    "#len(single_tfidf_value)\n",
    "\n",
    "#print(example_data_clear)\n",
    "\n",
    "# tf_idf_example = get_tf_idf_test(example_data_clear,all_word_count,total_rows)\n",
    "# pred = predict(model,tf_idf_example)\n",
    "\n",
    "#print(\"Dieser Artikel ist\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_s= single_tfidf_value\n",
    "similarity_dict = {}\n",
    "\n",
    "for j in range(len(tf_idf_train)):\n",
    "\n",
    "    query_s = list(tf_idf_train[j])[:-1]\n",
    "        \n",
    "    sim = 1 - cosine(line_s, query_s)\n",
    "\n",
    "    similarity_dict[j] = sim\n",
    "\n",
    "    similarity_dict = {k: v for k, v in sorted(similarity_dict.items(), key=lambda item: item[1], reverse = True)}\n",
    "    keys = list(similarity_dict.keys())[:21]\n",
    "    fake = 0\n",
    "    real = 0\n",
    "    for k in keys:\n",
    "        \n",
    "        tmp_class_label = int(list(tf_idf_train[k])[-1:][0])\n",
    "        #print(tmp_class_label)\n",
    "        if tmp_class_label == 0:\n",
    "            fake += 1\n",
    "        elif tmp_class_label == 1:\n",
    "            real += 1\n",
    "    #print(\"F\",fake,\" R\", real)\n",
    "if fake > real:\n",
    "    print(\"Der Nachrichtentext ist FAKE\")\n",
    "else:\n",
    "    print(\"Der Nachrichtentext ist TRUE\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weitere Ideen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Wahrscheinlichkeiten der Worte pro Klasse aggregieren/Durschnitt 1 Vektor pro Klasse 0,5>x True\n",
    "#{0:{word1:0.54,word2:0.4}, (0.54+0.4)/2 \n",
    "#1:{word1:0.46,word2:0.6}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
