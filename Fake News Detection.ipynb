{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load Data and make test- train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "            with open(file_name, 'r', encoding=\"utf8\") as csvfile:\n",
    "                csv_reader_object = csv.reader(csvfile, delimiter=',')\n",
    "                counter = 0\n",
    "                csv_list = []\n",
    "                for row in csv_reader_object:\n",
    "                    #print(row)\n",
    "                    if counter == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        csv_list.append(row+[file_name[7:11]])\n",
    "                    counter += 1\n",
    "            \n",
    "            print(counter-1,\"Eintr채ge aus\", file_name[7:], \"geladen\")\n",
    "            return csv_list        \n",
    "    else:\n",
    "        print(\"Datei\", file_name ,\"nicht gefunden\") \n",
    "        \n",
    "def main():\n",
    "    file_folder = \"./data/\"\n",
    "    files = [\"Fake.csv\",\"True.csv\"]\n",
    "\n",
    "    main_data = []\n",
    "\n",
    "    for element in files:\n",
    "        file_name = file_folder+element\n",
    "        main_data += (load_data(file_name))\n",
    "\n",
    "    print(\"Es gibt insgesamt\", len(main_data), \"Eintr채ge\")\n",
    "\n",
    "    random.shuffle(main_data)\n",
    "    train_data,test_data = train_test_split(main_data,test_size=0.2) \n",
    "\n",
    "    print(\"L채nge train_data:\", len(train_data),\" und L채nge test_data:\", len(test_data))\n",
    "    #print(train_data[0][3])\n",
    "\n",
    "    return train_data,test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocID(aFile):\n",
    "    #get a unique docID\n",
    "    prefix,fileName=os.path.split(aFile)\n",
    "    prefix,sentDir=os.path.split(prefix)\n",
    "    prefix,userDir=os.path.split(prefix)    \n",
    "    docID=userDir+\"|\"+sentDir+\"|\"+fileName;\n",
    "    return docID\n",
    "    \n",
    "def makeWordStats(aFile):    \n",
    "    global termBase\n",
    "    global numberDocsSeen\n",
    "    global docBase\n",
    "    \n",
    "    \n",
    "    alreadySeen=set()\n",
    "    \n",
    "    file=open(aFile,\"r\")\n",
    "    content=file.read()\n",
    "    wordCount=0\n",
    "    for s in content.split(\" \"):\n",
    "        s=s.lower()#lower case        \n",
    "        if not( \"\\n\" in s) and len(s)>3 and not(s in alreadySeen) and s.isalpha():\n",
    "            wordCount=wordCount+1            \n",
    "            alreadySeen.add(s)\n",
    "            if s in termBase:\n",
    "                termBase[s][\"numberDocsContainedTerm\"]=termBase[s][\"numberDocsContainedTerm\"]+1 #called only once per documente for a specific term (otherwise alreadySeen contains is)\n",
    "            else:\n",
    "                termBase[s]={\"numberDocsContainedTerm\":1}\n",
    "    docBase[getDocID(aFile)]={\"file\":aFile,\"wordCount\":wordCount,\"colIndex\":numberDocsSeen}\n",
    "    numberDocsSeen=numberDocsSeen+1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callCounter=0\n",
    "stopAfterMax=10000#set to low value like 50 reduces computation time\n",
    "#mode is preburn for colecting statitics and computing idf\n",
    "#mode is wordVector for computing the wordvector, preburn must have been called before\n",
    "def crawl(startDir):\n",
    "    #print(\"crawl with mode\",mode)\n",
    "    global callCounter\n",
    "    global stopAfterMax\n",
    "    \n",
    "    #print(callCounter,stopAfterMax)\n",
    "    for entry in os.listdir(startDir):\n",
    "        #print(entry)\n",
    "        candidate=startDir+\"/\"+entry        \n",
    "        if (os.path.isdir(candidate)):            \n",
    "            callCounter=callCounter+1\n",
    "            if callCounter<stopAfterMax:\n",
    "                crawl(candidate)\n",
    "                \n",
    "        else:#not a path, so index it            \n",
    "            callCounter=callCounter+1\n",
    "            if callCounter<stopAfterMax:\n",
    "                #print(candidate)                \n",
    "                makeWordStats(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callCounter=0\n",
    "\n",
    "termBase={}#a hashmap to count how many docs contain a specific term\n",
    "docBase={}#stores the number of terms in each document identifed by its docID and the full file name and the column positoin in the wordVector\n",
    "numberDocsSeen=0\n",
    "crawl(maildir_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build idf scores using a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeIDFfromstats():    \n",
    "    global idf_sorted\n",
    "    global termBase\n",
    "    for term in termBase.keys():\n",
    "        idf_d=-np.log(termBase[term][\"numberDocsContainedTerm\"]/numberDocsSeen)\n",
    "        termBase[term][\"idf\"]=idf_d\n",
    "        \n",
    "        idf_sorted.append((idf_d,term))\n",
    "    idf_sorted.sort(reverse=True)\n",
    "    counter=0\n",
    "    for idf_term in idf_sorted:\n",
    "        term=idf_term[1]        \n",
    "        termBase[term][\"rowIndex\"]=counter\n",
    "        counter=counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_sorted=[]#the mapping from term to idf sorted by idf just to inspect the result\n",
    "makeIDFfromstats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute word-vectors using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWordVector(aDocID):\n",
    "    \n",
    "    global wordVectors\n",
    "    global docBase\n",
    "    global termBase    \n",
    "    wordsInDocument={}#count how many times a specific term occured in the document\n",
    "    \n",
    "    \n",
    "    \n",
    "    fileName=docBase[aDocID][\"file\"]\n",
    "    colIndex=docBase[aDocID][\"colIndex\"]\n",
    "    file=open(fileName,\"r\")\n",
    "    content=file.read()\n",
    "    \n",
    "    #count words in document\n",
    "    for s in content.split(\" \"):\n",
    "        s=s.lower()#lower case\n",
    "        if not( \"\\n\" in s) and len(s)>3 and s.isalpha():                        \n",
    "            if s in wordsInDocument:\n",
    "                wordsInDocument[s]=wordsInDocument[s]+1\n",
    "            else:\n",
    "                wordsInDocument[s]=1\n",
    "    \n",
    "    #compute tf-idf per each term\n",
    "    for term in wordsInDocument:\n",
    "        termFreq=wordsInDocument[term]\n",
    "        invDocFreq=termBase[term][\"idf\"]\n",
    "        rowIndex=termBase[term][\"rowIndex\"]\n",
    "        #print(term,\"termFreq\",termFreq,\"invDocFreq\",invDocFreq,\"colIndex\",colIndex,\"rowIndex\",rowIndex)\n",
    "        wordVectors[rowIndex][colIndex]=termFreq*invDocFreq    \n",
    "        #termRowIndex=map_term_position[term]\n",
    "        #wordVectors[termRowIndex][numberDocsSeen]=termFreq*invDocFreq #make the entry for term and document\n",
    "    #docStatistics[getDocID(aFile)][2]=numberDocsSeen#store the column this document has in the wordVector-Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberRows=len(termBase.keys())\n",
    "numberCols=len(docBase.keys())\n",
    "#rows-> term (the rows are sorted according to idf_sorted, such that very frequent terms are at the bottom of the matrix)\n",
    "#cols-> doc\n",
    "wordVectors=np.zeros(shape=(numberRows,numberCols))\n",
    "print(\"shape of wordVectors:\",np.shape(wordVectors))\n",
    "\n",
    "for docID in docBase:\n",
    "    makeWordVector(docID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build statistics / learning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(aTweet,classified):\n",
    "    global termBase\n",
    "    alreadySeen=set()\n",
    "    splits=aTweet.split(\" \")\n",
    "    for token in splits:\n",
    "        token=token.lower()\n",
    "        if (len(token)>2) and token.isalpha() and (not(token in alreadySeen)):\n",
    "            alreadySeen.add(token)\n",
    "            if not(token in termBase):\n",
    "                termBase[token]={\"negative\":0,\"neutral\":0,\"positive\":0}\n",
    "            if classified==\"negative\":\n",
    "                termBase[token][\"negative\"]=termBase[token][\"negative\"]+1\n",
    "            elif classified==\"neutral\":\n",
    "                termBase[token][\"neutral\"]=termBase[token][\"neutral\"]+1\n",
    "            elif classified==\"positive\":\n",
    "                termBase[token][\"positive\"]=termBase[token][\"positive\"]+1\n",
    "            else:\n",
    "                print(\"class not supported\",classified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crawl through the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl():\n",
    "    for i in range (0,len(df)):\n",
    "    #for i in range (0,10):\n",
    "        classified=df.iloc[i].airline_sentiment\n",
    "        tweet=df.iloc[i].text\n",
    "        analyze(tweet,classified)\n",
    "        countPrior(classified)\n",
    "def countPrior(classified):\n",
    "    if classified==\"negative\":\n",
    "        prior[\"negative\"]=prior[\"negative\"]+1\n",
    "    elif classified==\"neutral\":\n",
    "        prior[\"neutral\"]=prior[\"neutral\"]+1\n",
    "    elif classified==\"positive\":\n",
    "        prior[\"positive\"]=prior[\"positive\"]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termBase={}\n",
    "prior={\"negative\":0,\"neutral\":0,\"positive\":0}\n",
    "crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(aText):    \n",
    "    product_neg=0\n",
    "    product_neu=0\n",
    "    product_pos=0\n",
    "    length=len(df)\n",
    "    for token in aText.split(\" \"):\n",
    "        token=token.lower()\n",
    "        if (len(token)>2) and token.isalpha() and isTokenNonZero(token):            \n",
    "            print(token)\n",
    "            product_neg=product_neg+np.log(termBase[token][\"negative\"]/prior[\"negative\"])\n",
    "            product_neu=product_neu+np.log(termBase[token][\"neutral\"]/prior[\"neutral\"])\n",
    "            product_pos=product_pos+np.log(termBase[token][\"positive\"]/prior[\"positive\"])\n",
    "    product_neg=product_neg+np.log(prior[\"negative\"]/length)\n",
    "    product_neu=product_neu+np.log(prior[\"neutral\"]/length)\n",
    "    product_pos=product_pos+np.log(prior[\"positive\"]/length)\n",
    "    print(\"negative\",product_neg)\n",
    "    print(\"neutral\",product_neu)\n",
    "    print(\"positive\",product_pos)\n",
    "\n",
    "def isTokenNonZero(token):\n",
    "    if not(token in termBase):\n",
    "        return False\n",
    "    if termBase[token][\"negative\"]<=0:\n",
    "        return False\n",
    "    if termBase[token][\"neutral\"]<=0:\n",
    "        return False\n",
    "    if termBase[token][\"positive\"]<=0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict(\"Beispiel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
